# -*- coding: utf-8 -*-
"""Movie_review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M4LqSJZeW1ITQkc8DFLtVUREDhYAjIHc

Installing NLTK
"""

pip install nltk==3.3

import nltk

"""Importing DATASET

"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

path = "/content/drive/MyDrive/IMDB Dataset.csv"
df_original = pd.read_csv(path,encoding='latin-1')
df_original.head()

df_original

df_original['sentiment'].value_counts()

"""Spliting Dataset"""

negative = []
positive = []
for i in range(0,len(df_original)):
  if(df_original['sentiment'][i] == 'negative'):
    negative.append(df_original['review'][i])
  elif(df_original['sentiment'][i] == 'positive'):
    positive.append(df_original['review'][i])

df_original['sentiment'][3]

print(negative[0])
print(positive[0])

"""Tokenizing Data"""

nltk.download('punkt')

negative_rating =[]
for i in range(len(negative)):
  words = nltk.word_tokenize(negative[i])
  negative_rating.append(words)

positive_rating =[]
for i in range(len(positive)):
  words = nltk.word_tokenize(positive[i])
  positive_rating.append(words)

positive_rating[0]

"""Removing Noise"""

nltk.download('stopwords')

from nltk.corpus import stopwords

from nltk.stem import PorterStemmer

import re,string

ps = PorterStemmer()
def remove_noise(tokens,stop_words):

  cleaned_tokens = []
  # stop_words = stopwords.words('english')
  for token in tokens:
    token = re.sub("[^a-zA-Z]"," " ,token)
    token = token.lower()
    token = token.split()
    token = [ps.stem(word) for word in token if word not in stop_words]
    token = " ".join(token)
    cleaned_tokens.append(token.lower())
  return cleaned_tokens

positive_cleaned_tokens = []
negative_cleaned_tokens = []
stop_words = stopwords.words('english')
for tokens in positive_rating:
  positive_cleaned_tokens.append(remove_noise(tokens,stop_words))

for tokens in negative_rating:
  negative_cleaned_tokens.append(remove_noise(tokens,stop_words))

"""Creating Dictionary """

from nltk import classify, NaiveBayesClassifier

def get_reviews_for_model(cleaned_tokens):
    for review_tokens in cleaned_tokens:
        yield dict([token, True] for token in review_tokens)

positive_tokens_for_model = get_reviews_for_model(positive_cleaned_tokens)
negative_tokens_for_model = get_reviews_for_model(negative_cleaned_tokens)

print(len(positive_cleaned_tokens))
print(len(negative_cleaned_tokens))

"""Model

"""

import random


positive_dataset = [(review_dict, "Positive") for review_dict in positive_tokens_for_model]

negative_dataset = [(review_dict, "Negative") for review_dict in negative_tokens_for_model]

dataset = positive_dataset + negative_dataset

random.shuffle(dataset)

train_data = dataset[:3500]
test_data = dataset[3500:]

classifier = NaiveBayesClassifier.train(train_data)


print("Accuracy is:", classify.accuracy(classifier, test_data))

"""Custom Input"""

custom_review = 'Great movie'
custom_tokens = remove_noise(nltk.word_tokenize(custom_review),stop_words)

print(classifier.classify(dict([token, True] for token in custom_tokens)))

custom_review = 'Worst movie'
custom_tokens = remove_noise(nltk.word_tokenize(custom_review),stop_words)

print(classifier.classify(dict([token, True] for token in custom_tokens)))